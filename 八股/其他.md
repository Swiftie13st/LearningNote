## 限流

漏桶和令牌桶都是常用的限流算法，用于控制系统的流量，防止系统被过度访问而崩溃。总的来说，漏桶算法适用于需要稳定处理请求的场景，而令牌桶算法适用于需要应对瞬时流量激增的场景。

### 固定窗口算法

劣势：临界时间点产生突发流量，统计数量不准确。

设每分钟请求数量为 60 个，每秒可以处理 1 个请求，用户在 00:59 发送 60 个请求，在 01:00 发送 60 个请求 此时 2 秒钟有 120 个请求(每秒 60 个请求)，远远大于了每秒钟处理数量的阈值。

### 滑动窗口算法 

滑动窗口算法弥补了计数器算法的不足。滑动窗口算法把间隔时间划分成更小的粒度，当更小粒度的时间间隔过去后，把过去的间隔请求数减掉，再补充一个空的时间间隔。

**当滑动窗口的格子划分的越多，滑动窗口的滚动就越平滑，限流的统计就会越精确。**

滑动窗口设置得越精细，限流的效果越好，但滑动窗口的时间间隔（小格子）多了，存储的空间也会增加。

### 漏桶

漏桶算法是一种固定容量的算法，类似于水桶，它可以以恒定的速率流出请求，无论输入速率有多快，最终输出速率都不会超过指定的限制。

漏桶算法的实现方式是，在服务端维护一个`固定容量`的队列（即漏桶），并以恒定速率处理请求。如果请求速率过快，请求将被加入到漏桶中，等待服务端处理。==当漏桶已满时，新的请求将被丢弃，从而控制了流量。==

漏桶算法的优点是实现简单，适用于需要**稳定处理请求**的场景，但是如果出现瞬时流量激增的情况，漏桶算法无法应对。

### 令牌桶

令牌桶算法的原理是系统会以一个恒定的速度往桶里放入令牌，而如果请求需要被处理，则需要先从桶里获取一个令牌，当桶里没有令牌可取时，则拒绝服务。
对于从桶里取不到令牌的场景，我们可以选择等待也可以直接拒绝并返回。

令牌桶算法的优点是可以应对**瞬时流量激增**的情况，因为它可以通过调整发放令牌的速率来控制请求的处理速度，但是实现相对漏桶算法更复杂一些。

limiter 并不是每隔一段时间更新当前桶的数量，而是记录了上次访问时和当前桶中令牌的数量，当再次访问时，通过上次访问时间计算出当前令牌的数量，决定是否可以发放。

```go
// The methods AllowN, ReserveN, and WaitN consume n tokens. 
type Limiter struct {
	mu sync.Mutex 
	limit Limit 
	burst int 
	tokens float64 // last is the last time the limiter's tokens field was updated 
	last time.Time // lastEvent is the latest time of a rate-limited event (past or future) 
	lastEvent time.Time 
}
```

后期使用Redis实现令牌桶算法：
使用lua脚本保证原子性
使用两个kv存储，一个存储当前数量，一个存储更新时间
记录当前key的值以及上次更新的时间，
定义返回值，是个数组，包含：是否触发限流（1限流 0通过）、当前桶中的令牌数

1. 首先判断当前令牌桶是否存在，可能是过期或刚启动
2. 如果不存在就创建桶
3. 存在就通过时间计算差计算当前剩余值减一放回，更新时间。
4. 如果桶中剩余数量小于零，返回限流

-   将获取令牌的操作封装在Lua脚本中。由于Lua脚本在redis中天然的原子性，可以实现我们的需求；
-   若太过依赖redis的话，我们可以每次请求redis时，预支一些令牌放在本地，通过本地的进程锁来分配这些令牌，消耗完毕在此请求redis。

Redis允许在Lua脚本中调用redis.call()或者redis.pcall()来执行Redis命令，如果Lua脚本对Redis的数据做了更改，那么除了执行脚本本身以外还需要两个额外的操作：

1.  把这段Lua脚本持久化到AOF文件中，保证Redis重启时可以回放执行过的Lua脚本。
2.  把这段Lua脚本复制给备库执行，保证主备库的数据一致性。

由于上述两步，现在就很容易理解为什么Redis要求Lua脚本必须是纯函数的形式了，想象一下给定一段Lua脚本和输入参数却得到了不同的结果，这就会造成重启前后和主备库之间的数据不一致，Redis不允许对数据一致性的破坏。

```lua
-- 定义返回值res[1]是否触发限流（1限流 0通过）res[2]当前桶中的令牌数
local res={}
res[1]=0
--local curtime=redis.call('time')
local inteval_time=tonumber(ARGV[1]) -- 放入令牌桶的间隔时间
local current_time=tonumber(ARGV[2]) -- 当前时间

local amount=10 -- 一次取几个
local key_expire_time=1000*3600 -- 过期时间
local inflow__per_unit=100 -- 每次放多少
local capacity=1000
local st_key='last_update'
local bucket_amount = 0

-- 上次向桶中投放令牌的时间
local last_time=redis.call('get',st_key)
-- 当前令牌数
local current_value = redis.call('get',KEYS[1])

if(last_time == false or current_value == false) -- 令牌桶也不存在或过期，重新生成令牌桶
then
    bucket_amount = capacity - amount;
    -- 生成令牌桶
    redis.call('set',KEYS[1],bucket_amount,'PX',key_expire_time)
    -- 设置投放时间
    redis.call('set',st_key,current_time,'PX',key_expire_time)
    res[2]=bucket_amount
    return res
end

current_value = tonumber(current_value)
last_time=tonumber(last_time)
local past_time=current_time-last_time --当前时间-上次投放的时间
if(past_time<inteval_time)
then
    -- 不到放入令牌时间，直接从令牌桶中取走令牌
    bucket_amount=current_value-amount
else
    -- 需要放入令牌
    local cur_times = past_time/inteval_time -- 放几次

    cur_times=math.floor(cur_times)

    bucket_amount=current_value+cur_times*inflow__per_unit
    if (bucket_amount > capacity)
    then
        bucket_amount = capacity-amount
    end
    -- 有新投放，更新投放时间
    redis.call('set',st_key,current_time,'PX',key_expire_time)
end

res[2]=bucket_amount

-- 触发限流
if(bucket_amount<0)
then
    res[1]=1
    return res
end

-- 更新令牌桶KV
redis.call('set',KEYS[1],bucket_amount,'PX',key_expire_time)
return res
```

## 鉴权JWT

JWT( JSON-WEB-TOKEN ) 是比较新的一种登录方式，他利用时间换空间的方式，服务端将用户的信息相关信息进行加密并返回到客户端，即签发了 一个"令牌"，在令牌的有效期内，客户端可以通过传递令牌的方式与服务端通信。JWT 登录整体流程如下：

- 用户输入用户名密码进行登录
- 服务端验证用户名密码，成功后，将用户的相关信息（通常是 user_id）及一些附加信息通过 JWT 方式进行加密，并返回给客户端。
- 客户端可以用任意方式储存服务器返回的 JWT ，之后只需在每次请求时，将 JWT 通过某种方式传递给后端。JWT默认的传递方式为:
```json
"headers": {
 'Authorization': 'Bearer ' + token // JWT 规定的的表示形式
}
```
- 服务器收到请求后，获取并验证 JWT，从而获取用户的信息（通常是 user_id 及一些附加信息），即服务器不需要储存每个用户的状态（即 session）， 只需要在每次请求时获取并解析 JWT，即可完成用户身份校验和用户基本信息的获取。

### 各部分信息

-   Header（头部）：alg：签名的算法；typ：token的类型，"JWT"
-   Payload（负载）
-   Signature（签名）：对前两部分的签名，防止数据篡改。

### 特点

1. JWT 默认是不加密，但也是可以加密的。生成原始 Token 以后，可以用密钥再加密一次。不加密的情况下，不要将秘密数据写入 JWT。
2. JWT 不仅可以用于认证，也可以用于交换信息。有效使用 JWT，可以降低服务器查询数据库的次数。
3. JWT 的最大缺点是，由于服务器不保存 session 状态，因此无法在使用过程中废止某个 token，或者更改 token 的权限。也就是说，一旦 JWT 签发了，在到期之前就会始终有效，除非服务器部署额外的逻辑。
4. JWT 本身包含了认证信息，一旦泄露，任何人都可以获得该令牌的所有权限。为了减少盗用，JWT 的有效期应该设置得比较短。对于一些比较重要的权限，使用时应该再次对用户进行认证。
5. 为了减少盗用，JWT 不应该使用 HTTP 协议明码传输，要使用 HTTPS 协议传输。
6. 后端每次接口请求都需要进行 JWT 的加解密，计算压力增大。

### 为什么要用两个，而不用一个token, refresh token的作用 

refreshToken就是用来在accessToken过期以后来重新获取accessToken的

-   用户在访问网站时，`accessToken`被盗取了，此时攻击者就可以拿这个`accessToke`访问权限以内的功能了。如果`accessToken`设置一个短暂的有效期2小时，攻击者能使用被盗取的`accessToken`的时间最多也就2个小时，除非再通过`refreshToken`刷新`accessToken`才能正常访问。
    
-   设置`accessToken`有效期是永久的，用户在更改密码之后，之前的`accessToken`也是有效的

### 为什么不用Cookie+Session方式？

`Session`是在服务端保存的一个数据结构，用来跟踪用户的状态，这个数据可以保存在集群、数据库、文件中；
`Cookie`是客户端保存用户信息的一种机制，用来记录用户的一些信息，也是实现Session的一种方式。

cookie + session 是最传统的登录方式，利用浏览器默认行为，每次请求将登录后设置好的 cookie 发送给服务端， 服务端通过 cookie 中的信息（ session_id），获取用户的登录信息。整体流程如下：

- 用户输入用户名密码进行登录
- 服务端验证用户名密码，成功后，生成唯一的 session_id 储存起来（可以是内存、数据库等，通常使用 redis ）。
- 通过设置 set-cookie，将 session_id 返回给前端并储存在浏览器 cookie 中。
- cookie 过期前，对该系统的每次请求都将会带上 cookie（浏览器默认行为），后端通过 cookie 中的信息，获取用户的 session_id 信息。 并在后端（ redis ）查询出对应用户的信息。

优点 ：原理简单、实现方便

**缺点**:

- **服务器端需维护大量 session_id**，有一定负担。（目前通常将 session_id 放在 redis中，也解决了服务器集群下 session_id 同步问题）
- 无法阻止[跨站请求伪造**CSRF**](https://zh.m.wikipedia.org/zh/跨站请求伪造) 攻击。
- 这种模式的问题在于，扩展性（scaling）不好。单机当然没有问题，如果是服务器集群，或者是跨域的服务导向架构，就要求 session 数据共享，每台服务器都能够读取 session。

## 异步写库

实现异步写库可以帮助减轻数据库的压力，提高系统的吞吐量和响应速度。在每次写入数据库操作时，不直接写入到MySQL数据库中，防止在高并发情况下数据库瓶颈出现。为了解决这个问题，我把所有的更新、插入数据库的需求，放入一个独立的goroutine中，使用channel进行数据的异步传递，也避免了多个goroutine之间的竞争和锁的使用，使代码更加简洁易于维护。

### 异步写库会带来一些数据一致性的问题，如何保证数据的一致性？

采用**先更新数据库再删除缓存**的方案
缺点：假如缓存删除失败或者更新数据库时，其他线程读到缓存的旧数据。但是仅是出现读取到一次旧值，并不会对主要数据产生影响。

### 保证数据一致性回写redis要注意什么？

先更新数据库再删除缓存，使用**双检加锁机制**锁住mysql，只让一个线程回写redis，完成数据一致性。

### 双检加锁

多个线程同时查询数据库的一条数据时，在第一个查询数据的请求上使用一个互斥锁来锁住它，其他线程走到这一步拿不到锁，就等到第一个线程查询到数据后，然后做缓存。其他线程发现有缓存后就不会去读取mysql。

双检加锁就是在加锁后再次查询reids缓存，二次查询无数据才去查询mysql

### 如果存在更新频繁的热点key怎么办

同一热点key保留2份，A有过期时间，B无

先查询A的，查询不到，则：
1.  后端查询DB更新缓存
2.  查询带后缀返回给调用方

### 延迟双删用过吗？会有哪些问题

延迟双删用于先删除redis缓存再更新mysql的策略中：

延迟双删：==先删除缓存在写数据库再延迟一段时间再次删除缓存==

#### why？

假设A删除完redis缓存，然后更新mysql，
此时B读取数据，redis找不到，读mysql然后回写到redis中
A再次回写redis出现问题

因此在A第一次删除缓存时，延迟一段时间再次删除

#### 延迟时间

1. 估算一次读取数据然后写入缓存中的时间，在此基础上加几百毫秒，确保读请求结束，写请求可以删除读请求造成的缓存脏数据
2. 启动一个后台监控程序，比如WatchDog

### 删除缓存失败怎么办？

1. 可以把要删除的缓存值或者要更新的数据库值暂存到消息队列中（Kafka，RabbitMQ等）
2. 当程序没有删除缓存或者更新数据库时，可以从消息队列中读取这些值，然后再次进行删除或更新。
3. 如果成功删除或更新，就从消息队列中弹出这些值，防止重复操作。此时如果还是失败就再次重试
4. 如果重试一定次数还没成功，就要发送报错信息，通知运维

流程：
1. 更新数据库
2. 数据库写入binlog
3. 订阅程序从binlog中提取出信息
4. 另起一段非业务代码，获取该信息
5. 尝试删除缓存，发现删除失败
6. 将这些信息发送到消息队列
7. 从消息队列中获取该数据，执行重试操作


如果缓存删除失败，就会出现缓存与数据库数据不一致的情况。不过删除行为是幂等的，可以通过重试机制来保证缓存中的数据终将被删除。

### 如何保证最终一致性？

给缓存设置过期时间，定期清理并回写。
以mysql的数据库写入库为准，对缓存操作尽最大努力即可。

#### 如果业务层必须保证读取一致性？

在更新数据库时先暂停Redis缓存并发读请，等操作完删除缓存后再读取数据。

#### 如果想要mysql有改动，立即同步redis如何做？

使用阿里巴巴canal，会读取mysql的binlog

MySQL binlog增量订阅消费+消息队列+增量数据更新到redis读Redis

热数据基本都在Redis写MySQL:增删改都是操作MySQL更新Redis数据：MySQ的数据操作binlog，来更新到Redis：

1)数据操作主要分为两大块：一个是全量(将全部数据一次写入到redis)一个是增量(实时更新)。

这里说的是增量,指的是mysql的update、insert、delate变更数据。

2)读取binlog后分析 ，利用消息队列,推送更新各台的redis缓存数据。
这样一旦MySQL中产生了新的写入、更新、删除等操作，就可以把binlog相关的消息推送至Redis，Redis再根据binlog中的记录，对Redis进行更新。
其实这种机制，很类似MySQL的主从备份机制，因为MySQL的主备也是通过binlog来实现的数据一致性。
这里可以结合使用canal(阿里的一款开源框架)，通过该框架可以对MySQL的binlog进行订阅，而canal正是模仿了mysql的slave数据库的备份请求，使得Redis的数据更新达到了相同的效果。
当然，这里的消息推送工具你也可以采用别的第三方：kafka、rabbitMQ等来实现推送更新Redis。

## 大数据场景

[十道海量数据处理面试题与十个方法大总结_v_JULY_v的博客-CSDN博客](https://blog.csdn.net/v_july_v/article/details/6279498)

https://interviewguide.cn/notes/03-hunting_job/02-interview/07-01-massive_data.html

https://interviewguide.cn/notes/03-hunting_job/02-interview/07-02-massive_data.html
### 从M亿个整数里找出X个最大的数 

1. 先对这批海量数据预处理，在O(N)的时间内用Hash表进行拆分映射
2. 将需要排序的数据切分为多个样本数大致相等的区间，例如：1-100，101-300… 这里要考虑IO次数和硬件资源问题，例如可将小数据文件数设定为1G（要预留内存给执行时的程序使用） 
3. 读取每个小文件的前100个数字，建立最大值堆。（这里采用堆排序将空间复杂度讲得很低，要排序1亿个数，但一次性只需读取100个数字，或者设置其他基数，不需要1次性读完所有数据，降低对内存要求） 
4. 依次读取余下的数，与最大值堆作比较，维持最大值堆。可以每次读取的数量为一个磁盘页面，将每个页面的数据依次进堆比较，这样节省IO时间。 
5. 将堆进行排序，即可得到100个有序最大值。
6. 最后对各个数据区间内的排序结果文件进行处理，最终每个区间得到一个排序结果的文件，将各个区间的排序结果合并。 

### 从1Tb单词中找到词频最高的单词 

1. 分而治之，进行哈希取余；
2. 使用 HashMap 统计频数；
3. 求解**最大**的 TopN 个，用**小顶堆**；求解**最小**的 TopN 个，用**大顶堆**。

1) 1G文件远远大于1M内存，分治法，先hash映射把大文件分成很多个小文件，具体操作如下：读文件中，对于每个词x，取hash(x)%5000，然后按照该值存到5000个小文件(记为f0,f1,...,f4999)中，这样每个文件大概是200k左右（每个相同的词一定被映射到了同一文件中）
2) 对于每个文件fi，都用hash_map做词和出现频率的统计，取出频率大的前100个词（怎么取？topK问题，建立一个100个节点的最小堆），把这100个词和出现频率再单独存入一个文件
3) 根据上述处理，我们又得到了5000个文件，归并文件取出top100（Top K 问题，比较最大的前100个频数）

### 1TB数据用4GB内存排序，描述细节

外排序采用分块的方法（分而治之），**首先将数据分块**，对块内数据按选择一种高效的内排序策略进行排序，**块内可以用快排**。然后采用**归并排序**的思想对于所有的块进行排序，得到所有数据的一个有序序列

### 1TB数据，100个不重复的值排序

100个文件，每个文件放一种值

### 快速排序很多重复数字如何优化

答：返回基准元素位置时返回基准元素的最左和最有索引，减少排序次数。

### 10个数组，求这10个数组中最大的十个数 

维护一个容量为10的小根堆，当前树大于根节点时，将根节点pop掉，新节点加入

### 10亿重复个数据，如何找出只出现一个的数据？hash为什么不能用？ 

使用两个bitmap

然后我们怎么统计只出现一次的数呢？每一个数出现的情况我们可以分为三种：0次、1次、大于1次。也就是说我们需要用2个bit位才能表示每个数的出现情况。此时则三种情况分别对应的bit位表示是：00、01、11

我们顺序扫描这10亿的数，在对应的双bit位上标记该数出现的次数。最后取出所有双bit位为01的int型数就可以了。

### 1亿个QQ号中快速找到出现频率最高的那个 

由于内存限制，我们依然无法直接将大文件的所有词一次读到内存中。因此，同样可以采用**分治策略**，把一个大文件分解成多个小文件，保证每个文件的大小小于 1MB，进而直接将单个小文件读取到内存中进行处理。

**思路如下**：

首先遍历大文件，对遍历到的每个词 x，执行 `hash(x) % 5000` ，将结果为 i 的词存放到文件 ai 中。遍历结束后，我们可以得到 5000 个小文件。每个小文件的大小为 200KB 左右。如果有的小文件大小仍然超过 1MB，则采用同样的方式继续进行分解。

接着统计每个小文件中出现频数最高的 100 个词。最简单的方式是使用 HashMap 来实现。其中 key 为词，value 为该词出现的频率。具体方法是：对于遍历到的词 x，如果在 map 中不存在，则执行 `map.put(x, 1)` ；若存在，则执行 `map.put(x, map.get(x)+1)` ，将该词频数加 1。

上面我们统计了每个小文件单词出现的频数。接下来，我们可以通过维护一个**小顶堆**来找出所有词中出现频数最高的 100 个。具体方法是：依次遍历每个小文件，构建一个**小顶堆**，堆大小为 100。如果遍历到的词的出现次数大于堆顶词的出现次数，则用新词替换堆顶的词，然后重新调整为**小顶堆**，遍历结束后，小顶堆上的词就是出现频数最高的 100 个词。

1. 分而治之，进行哈希取余；
2. 使用 HashMap 统计频数；
3. 求解**最大**的 TopN 个，用**小顶堆**；求解**最小**的 TopN 个，用**大顶堆**。

### 位图法（Bitmap） 

[【算法】10亿int型数，统计只出现一次的数_Mlib的博客-CSDN博客](https://blog.csdn.net/u010983881/article/details/75097358)

位图法是基于int型数的表示范围这个概念的，用一个bit位来标识一个int整数，若该位为1，则说明该数出现；若该位为0，则说明该数没有出现。一个int整型数占4字节（Byte），也就是32位（bit）。那么把所有int整型数字表示出来需要2^32 bit位的空间，为了方便，我们可以把这些信息每8bit分割保存为byte数组，换算成字节单位也就是2^32 bit/8 = 2^29 Byte，大约等于512MB

如果有多个维度，则开多个bitmap即可

优点：

- 排序、查找、去重等运算效率高
- 占用空间低

缺点：

- 结果数据不能重复
- 如果数据较为离散则会造成空间浪费。但是也有开源工具实现了压缩算法

### 两个大文件中找出共同记录

（1）首先我们最常想到的方法是读取文件a，建立哈希表（为什么要建立hash表？因为方便后面的查找），然后再读取文件b，遍历文件b中每个url，对于每个遍历，我们都执行查找hash表的操作，若hash表中搜索到了，则说明两文件共有，存入一个集合。

（2）但上述方法有一个明显问题，加载一个文件的数据需要50亿\*64bytes = 320G远远大于4G内存，何况我们还需要分配哈希表数据结构所使用的空间，所以不可能一次性把文件中所有数据构建一个整体的hash表。

（3）针对上述问题，我们分治算法的思想:

step1：遍历文件a，对每个url求取hash(url)%1000，然后根据所取得的值将url分别存储到1000个小文件(记为a0,a1,...,a999，每个小文件约300M)，为什么是1000？主要根据内存大小和要分治的文件大小来计算，我们就大致可以把320G大小分为1000份，每份大约300M（当然，到底能不能分布尽量均匀，得看hash函数的设计）

step2：遍历文件b，采取和a相同的方式将url分别存储到1000个小文件(记为b0,b1,...,b999)（为什么要这样做? 文件a的hash映射和文件b的hash映射函数要保持一致，这样的话相同的url就会保存在对应的小文件中，比如，如果a中有一个url记录data1被hash到了a99文件中，那么如果b中也有相同url，则一定被hash到了b99中）

所以现在问题转换成了：找出1000对小文件中每一对相同的url（不对应的小文件不可能有相同的url）

step3：因为每个hash大约300M，所以我们再可以采用（1）中的想法

#### 共同url

给定 a、b 两个文件，各存放 50 亿个 URL，每个 URL 各占 64B，内存限制是 4G。请找出 a、b 两个文件共同的 URL。

每个 URL 占 64B，那么 50 亿个 URL 占用的空间大小约为 320GB。

> 5, 000, 000, 000 _ 64B ≈ 5GB _ 64 = 320GB

由于内存大小只有 4G，因此，我们不可能一次性把所有 URL 加载到内存中处理。对于这种类型的题目，一般采用**分治策略**，即：把一个文件中的 URL 按照某个特征划分为多个小文件，使得每个小文件大小不超过 4G，这样就可以把这个小文件读到内存中进行处理了。

首先遍历文件 a，对遍历到的 URL 求 `hash(URL) % 1000` ，根据计算结果把遍历到的 URL 存储到 a0, a1, a2, ..., a999，这样每个大小约为 300MB。使用同样的方法遍历文件 b，把文件 b 中的 URL 分别存储到文件 b0, b1, b2, ..., b999 中。这样处理过后，所有可能相同的 URL 都在对应的小文件中，即 a0 对应 b0, ..., a999 对应 b999，不对应的小文件不可能有相同的 URL。那么接下来，我们只需要求出这 1000 对小文件中相同的 URL 就好了。

接着遍历 ai( `i∈[0,999]` )，把 URL 存储到一个 HashSet 集合中。然后遍历 bi 中每个 URL，看在 HashSet 集合中是否存在，若存在，说明这就是共同的 URL，可以把这个 URL 保存到一个单独的文件中。

也可以用**前缀树**：一般而言，URL 的长度差距不会不大，而且前面几个字符，绝大部分相同。这种情况下，非常适合使用**字典树**（trie tree） 这种数据结构来进行存储，降低存储成本的同时，提高查询效率。
- 利用字符串的公共前缀来降低存储成本，提高查询效率。

### 如何查询最热门的查询串？

搜索引擎会通过日志文件把用户每次检索使用的所有查询串都记录下来，每个查询串的长度不超过 255 字节。

假设目前有 1000w 个记录（这些查询串的重复度比较高，虽然总数是 1000w，但如果除去重复后，则不超过 300w 个）。请统计最热门的 10 个查询串，要求使用的内存不能超过 1G。（一个查询串的重复度越高，说明查询它的用户越多，也就越热门。）


每个查询串最长为 255B，1000w 个串需要占用 约 2.55G 内存，因此，我们无法将所有字符串全部读入到内存中处理。

#### 方法一：分治法

分治法依然是一个非常实用的方法。

划分为多个小文件，保证单个小文件中的字符串能被直接加载到内存中处理，然后求出每个文件中出现次数最多的 10 个字符串；最后通过一个小顶堆统计出所有文件中出现最多的 10 个字符串。

方法可行，但不是最好，下面介绍其他方法。

#### 方法二：HashMap 法

虽然字符串总数比较多，但去重后不超过 300w，因此，可以考虑把所有字符串及出现次数保存在一个 HashMap 中，所占用的空间为 300w*(255+4)≈777M（其中，4 表示整数占用的 4 个字节）。由此可见，1G 的内存空间完全够用。

**思路如下**：

首先，遍历字符串，若不在 map 中，直接存入 map，value 记为 1；若在 map 中，则把对应的 value 加 1，这一步时间复杂度 `O(N)` 。

接着遍历 map，构建一个 10 个元素的小顶堆，若遍历到的字符串的出现次数大于堆顶字符串的出现次数，则进行替换，并将堆调整为小顶堆。

遍历结束后，堆中 10 个字符串就是出现次数最多的字符串。这一步时间复杂度 `O(Nlog10)` 。

#### 方法三：前缀树法

方法二使用了 HashMap 来统计次数，当这些字符串有大量相同前缀时，可以考虑使用前缀树来统计字符串出现的次数，树的结点保存字符串出现次数，0 表示没有出现。

**思路如下**：

在遍历字符串时，在前缀树中查找，如果找到，则把结点中保存的字符串次数加 1，否则为这个字符串构建新结点，构建完成后把叶子结点中字符串的出现次数置为 1。

最后依然使用小顶堆来对字符串的出现次数进行排序。

