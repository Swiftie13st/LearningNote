# 分布式

## 分布式锁

一个最基本的分布式锁需要满足：

- **互斥** ：任意一个时刻，锁只能被一个线程持有；
- **高可用** ：锁服务是高可用的。当提供分布式锁服务的基础组件中存在少量节点发生故障时，不应该影响到分布式锁服务的稳定性。
- **可重入**：一个节点获取了锁之后，还可以再次获取锁。
- **健壮性**：即不能产生死锁（dead lock）. 假如某个占有锁的使用方因为宕机而无法主动执行解锁动作，锁也应该能够被正常传承下去，被其他使用方所延续使用.即使客户端的释放锁的代码逻辑出现问题，锁最终一定还是会被释放，不会影响其他线程对共享资源的访问。

### 分类

分布式锁根据其实现模型，可以被划分为两大类：

- **主动轮询型**：该模型类似于单机锁中的主动轮询 + CAS 乐观锁模型，取锁方会持续对分布式锁发出尝试获取动作，如果锁已被占用则会不断发起重试，直到取锁成功为止
-  **Watch 回调型**：在取锁方发现锁已被他人占用时，会创建 watcher 监视器订阅锁的释放事件，随后不再发起主动取锁的尝试；当锁被释放后，取锁方能通过之前创建的 watcher 感知到这一变化，然后再重新发起取锁的尝试动作

>然而，在分布式场景中，我个人觉得优势的天平在略微朝着 watch 回调型的实现策略倾斜. 这是因为分布式场景中”轮询“这一动作的成本相比于单机锁而言要高很多，背后存在的行为可能是一次甚至多次网络 IO 请求. 这种情况下，取锁方基于 watch 回调的方式，在确保锁被释放、自身有机会取锁的情况下，才会重新发出尝试取锁的请求，这样能在很大程度上避免无意义的轮询损耗.
>当然，主动轮询型的分布式锁能够保证使用方始终占据流程的主动权，整个流程可以更加轻便灵活；此外，watch 机制在实现过程中需要建立长连接完成 watch 监听动作，也会存在一定的资源损耗. 因此这个问题没有标准答案，应该结合实际的需求背景采取不同的应对策略：在并发激烈程度较高时倾向于 watch 回调型分布式锁；反之，主动轮询型分布式锁可能会是更好的选择.
>除此之外，基于 watch 回调模型实现的分布式锁背后可能还存在其他的问题，比如：当有多个尝试取锁的使用方 watch 监听同一把锁时，一次锁的释放事件可能会引发“惊群效应”

#### 主动轮询型

- 针对于同一把分布式锁，使用同一条数据进行标识（以 redis 为例，则为同一个 key 对应的 kv 数据记录）
- 假如在存储介质成功插入了该条数据（要求之前该 key 对应的数据不存在），则被认定为加锁成功
- 把从存储介质中删除该条数据这一行为理解为释放锁操作
- 倘若在插入该条数据时，发现数据已经存在（锁已被他人持有），则持续轮询，直到数据被他人删除（他人释放锁），并由自身完成数据插入动作为止（取锁成功）
- 由于是并发场景，需要保证【（1）检查数据是否已被插入（2）数据不存在则插入数据】这两个步骤之间是原子化不可拆分的（在 redis 中是 set only if not exist —— SETNX 操作）

#### Watch回调型

对于实现 watch 回调型分布式锁，一些基本要点和 主动轮询型分布式锁类似：

- 针对于同一把分布式锁，使用一条相同的数据进行标识（唯一、明确的 key）
- 倘若在存储介质内成功插入该条数据（要求 key 对应的数据不存在），则这一行为被认定为加锁成功
- 把从存储介质中删除该条数据这行为理解为解锁操作

与主动轮询型分布式锁不同的是，在取锁失败时，watch 回调型分布式锁不会持续轮询，而是会 watch 监听锁的删除事件：

- 倘若在插入数据时，发现该条记录已经存在，说明锁已被他人持有，此时选择监听这条数据记录的删除事件，当对应事件发生时说明锁被释放了，此时才继续尝试取锁

### mysql实现

#### 基于唯一索引实现

-   获取锁时在数据库中insert一条数据，包括id、方法名(唯一索引)、线程名(用于重入)、重入计数
-   获取锁如果成功则返回true
-   获取锁的动作放在while循环中，周期性尝试获取锁直到结束或者可以定义方法来限定时间内获取锁
-   释放锁的时候，delete对应的数据

#### 基于排他锁

-   获取锁可以通过，在select语句后增加`for update`，数据库会在查询过程中给数据库表增加排他锁。当某条记录被加上排他锁之后，其他线程无法再在该行记录上增加排他锁，我们可以认为获得排它锁的线程即可获得分布式锁；
-   其余实现与使用唯一索引相同；
-   释放锁通过`connection.commit();`操作，提交事务来实现。

### Redis实现

基于共享存储的分布式锁是指将锁存储在共享存储中，例如 Redis、ZooKeeper 等。多个进程或节点都可以访问这个共享存储，并通过协调机制来实现互斥访问。

常见的实现方式是使用 Redis 的 `SETNX` 命令，在共享存储中创建一个键，并设置过期时间，只有一个进程能够成功设置该键，其他进程则等待。当持有锁的进程完成操作后，需要删除该键以释放锁。

这种实现方式的优点是简单易用，但是需要保证共享存储的高可用性和性能，并且如果进程持有锁的时间过长，可能会导致其他进程等待的时间过长，影响系统性能。

此外，redis 还支持使用 lua 脚本自定义组装同一个 redis 节点下的多笔操作形成一个具备原子性的事务.

SETNX 是『SET if Not eXists』 (如果不存在，则 SET)的简写。 返回值 命令在设置成功时返回 1 ， 设置失败时返回 0。

#### 使用

1. 使用setnx上锁，通过del释放锁
2. 锁一直没有释放，设置过期时间，自动释放
3. 上锁之后突然出现异常，无法设置过期时间了
	上锁时候同时设置过期时间即可

```sql
set sku:1:info “OK” NX PX 10000
```

- `EX` second ：设置键的过期时间为 second 秒。 SET key value EX second 效果等同于 SETEX key second value 。
- `PX` millisecond ：设置键的过期时间为 millisecond 毫秒。 SET key value PX millisecond 效果等同于 PSETEX key millisecond value 。
- `NX` ：只在键不存在时，才对键进行设置操作。 SET key value NX 效果等同于 SETNX key value 。
- `XX` ：只在键已经存在时，才对键进行设置操作。

#### 注意

setnx获取锁时，设置一个指定的唯一值（例如：uuid）；释放前获取这个值，判断是否自己的锁.

- 不使用`DEL`命令来释放锁，而是发送一个**Lua脚本**，这个脚本只在客户端传入的值和键的口令串相匹配，才对键进行删除。

```lua
// 释放锁时，先比较锁对应的 value 值是否相等，避免锁的误释放 
if redis.call("get",KEYS[1]) == ARGV[1] then 
	return redis.call("del",KEYS[1]) 
else 
	return 0 
end
```

为了避免锁无法被释放，我们可以想到的一个解决办法就是： **给这个 key（也就是锁） 设置一个过期时间** 。

#### 基于Redis的分布式锁的实现思路

1. 利用`setnx uuid ex time`获取锁，并设置过期时间和保存线程标识uuid
2. 如果是集群状态，为防止当前主机上锁后宕机，要等待至少一半集群同意才返回上锁成功
3. 释放锁时使用Lua脚本，先判断线程标识uuid是否与自己一致，一致则删除锁

特性：
1. 利用set nx满足互斥性
2. 利用set ex保证故障时锁依然能释放，避免死锁，提高安全性
3. 利用Redis集群保证高可用性和高并发特性

#### 死锁问题

使用 redis 时，我们可以通过过期时间 expire time 机制得以保证. 我们通常会在插入分布式锁对应的 kv 数据时设置一个过期时间 expire time，这样即便使用方因为异常原因导致无法正常解锁，锁对应的数据项也会在达到过期时间阈值后被自动删除，实现释放分布式锁的效果.

值得一提的是，这种过期机制的引入也带来了新的问题：因为锁的持有者并不能精确预判到自己持锁后处理业务逻辑的实际耗时，因此此处设置的过期时间只能是一个偏向于保守的经验值，假如因为一些异常情况导致占有锁的使用方在业务处理流程中的耗时超过了设置的过期时间阈值，就会导致锁被提前释放，其他取锁方可能取锁成功，最终引起数据不一致的并发问题.

针对于这个问题，在分布式锁工具 redisson 中给出了解决方案——看门狗策略（watch dog strategy）：在锁的持有方未完成业务逻辑的处理时，会持续对分布式锁的过期阈值进行延期操作.

##### 看门狗策略

- 在执行 redis 分布式锁的上锁操作时，通过 setNEX 指令完成锁数据的设置，携带了一个默认的锁数据过期时间
- 确认上锁成功后，异步启动一个 watchDog 守护协程，按照锁默认过期时间 1/4 ~ 1/3 的节奏（可自由设置），持续地对锁数据进行 expire 续期操作
- 在解锁成功后，会负责关闭 watchDog，回收协程资源. （由于看门狗续期操作会先检查锁的所有权再延期数据，因此实际上使用方只要删除了锁数据，续期操作就不会生效了. 回收看门狗协程是为了规避协程泄漏问题）

#### 弱一致性问题

回顾 redis 的设计思路，为避免单点故障问题，redis 会基于主从复制的方式实现数据备份. （以哨兵机制为例，哨兵会持续监听 master 节点的健康状况，倘若 master 节点发生故障，哨兵会负责扶持 slave 节点上位，以保证整个集群能够正常对外提供服务）. 此外，在 CAP 体系中，redis 走的是 AP 路线，为保证服务的吞吐性能，主从节点之间的数据同步是异步延迟进行的.

到这里问题就来了，试想一种场景：倘若 使用方 A 在 redis master 节点加锁成功，但是对应的 kv 记录在同步到 slave 之前，master 节点就宕机了. 此时未同步到这项数据的 slave 节点升为 master，这样分布式锁被 A 持有的“凭证” 就这样凭空消失了. 于是不知情的使用方 B C D 都可能加锁成功，于是就出现了一把锁被多方同时持有的问题，导致分布式锁最基本的独占性遭到破坏.

关于这个问题，一个比较经典的解决方案是：redis 红锁（redlock，全称 redis distribution lock）

##### red lock 实现原理

所谓多数派原则，就是做出一项决议之前，让所有的参与者进行投票表决，只有投赞同票的人数达到参与者总人数的一半以上成为多数派时，这项决议才被通过.

在红锁 RedLock 实现中，会基于多数派准则进行 CAP 中一致性 C 和可用性 A 之间矛盾的缓和，保证在 RedLock 下所有 redis 节点中达到半数以上节点可用时，整个红锁就能够正常提供服务.

在红锁的实现中：

- 我们假定集群中有 2N+1个 redis 节点（通常将节点总数设置为奇数，有利于多数派原则的执行效率）
- 这些 redis 节点彼此间是相互独立的，不存在从属关系
- 每次客户端尝试进行加锁操作时，会同时对2N+1个节点发起加锁请求
- 每次客户端向一个节点发起加锁请求时，会设定一个很小的请求处理超时阈值
- 客户端依次对2N+1个节点发起加锁请求，只有在小于请求处理超时阈值的时间内完成了加锁操作，才视为一笔加锁成功的请求
- 过完2N+1个节点后，统计加锁成功的请求数量
- 倘若加锁请求成功数量大于等于N+1（多数派），则视为红锁加锁成功
- 倘若加锁请求成功数量小于N+1，视为红锁加锁失败，此时会遍历2N+1个节点进行解锁操作，有利于资源回收，提供后续使用方的取锁效率

## CAP理论

**CAP** 也就是 **Consistency（一致性）**、**Availability（可用性）**、**Partition Tolerance（分区容错性）** 这三个单词首字母组合。

- `一致性`(Consistency) :等同于所有节点访问同一份最新的数据副本，即更新操作成功后，所有节点再同一时间的数据一致。
- `可用性`（Availability)︰每次请求都能获取到非错的响应——但不能保证获取的数据为最新数据。即服务一直可用，不会出现用户操作失败或超时等体验不好的情况。
- `分区容错性`(Partition Tolerance)∶以实际效果而言，分区相当于对通信的时限要求。系统如果不能在时限内达成数据一致性，就意味着发生了分区的情况，必须就当前操作在C和A之间做出选择。(更易懂的解释:分区容忍性是尽管任意数量的消息被节点间的网络丢失或延迟，系统仍然正常运行)

对于一个分布式系统来说，不可能同时满足以上三点。并不是说只做两个，而是说需要选择一个进行降级。比如CP+高可用性
分区容错是必须要保证的，当发生网络分区时，如果要继续服务，那么强一致性和可用性只能2选1

### 一致性(Consistency)

一致性是指写操作后的读操作可以读取到最新的数据状态，当数据分布在多个节点上，从任意结点读取到的数据都是最新的状态。

1.  由于存在数据同步的过程，写操作的响应会有一定的延迟。
2.  为了保证数据一致性会对资源暂时锁定，待数据同步完成释放锁定资源。
3.  如果请求数据同步失败的结点则会返回错误信息，一定不会返回旧数据。

### 可用性(Availability)

服务一直可用，而且是正常响应时间。所有请求都有响应，且不会出现响应超时或响应错误。

1.  写入Master主数据库后要将数据同步到从数据库。
2.  由于要保证Backup从数据库的可用性，不可将Backup从数据库中的资源进行锁定。
3.  即使数据还没有同步过来，从数据库也要返回要查询的数据，哪怕是旧数据/或者默认数据，但不能返回错误或响应超时。

### 分区容错性(Partition tolerance)

分布式系统中，尽管部分节点出现任何消息丢失或者故障，系统应继续运行。**分区容忍性分是布式系统具备的基本能力**。
通常分布式系统的各各结点部署在不同的子网，这就是网络分区，不可避免的会出现由于网络问题而导致结点之间通信失败，此时仍可对外提供服务。

1.  尽量使用异步取代同步操作，例如使用异步方式将数据从主数据库同步到从数据，这样结点之间能有效的实现松耦合。
2.  添加Backup从数据库结点，其中一个Backup从结点挂掉其它Backup从结点提供服务。

### 总结

`CA` 放弃 P：如果不要求P（不允许分区），则C（强一致性）和A（可用性）是可以保证的。这样分区将永远不会存在，因此CA的系统更多的是允许分区后各子系统依然保持CA。

`CP` 放弃 A：如果不要求A（可用），相当于每个请求都需要在Server之间强一致，而P（分区）会导致同步时间无限延长，如此CP也是可以保证的。很多传统的数据库分布式事务都属于这种模式。

`AP` 放弃 C：要高可用并允许分区，则需放弃一致性。一旦分区发生，节点之间可能会失去联系，为了高可用，每个节点只能用本地数据提供服务，而这样会导致全局数据的不一致性。现在众多的NoSQL都属于此类。

### 按照CAP理论如何设计一个电商系统？

首先个电商网站核心模块有**用户，订单，商品，支付，促销管理**等

1. 对于用户模块，包括登录，个人设置，个人订单，购物车，收藏夹等，这些模块保证AP，数据短时间不一致不影响使用。  
2. 订单模块的下单付款扣减库存操作是整个系统的核心，CA都需要保证，极端情况下面牺牲A保证C  
3. 商品模块的商品上下架和库存管理保证CP  
4. 搜索功能因为本身就不是实时性非常高的模块，所以保证AP就可以了。  
5. 促销是短时间的数据不一致，结果就是优惠信息看不到，但是已有的优惠要保证可用，而且优惠可以提前预计算，所以可以保证AP。  
6. 支付这一块是独立的系统，或者使用第三方的支付宝，微信。其实CAP是由第三方来保证的，支付系统是一个对CAP要求极高的系统，C是必须要保证的，AP中A相对更重要，不能因为分区，导致所有人都不能支付

## BASE理论

**BASE** 是 **Basically Available（基本可用）** 、**Soft-state（软状态）** 和 **Eventually Consistent（最终一致性）** 三个短语的缩写。BASE 理论是对 CAP 中一致性 C 和可用性 A 权衡的结果，其来源于对大规模互联网系统分布式实践的总结，是基于 CAP 定理的AP逐步演化而来的，它大大降低了我们对系统的要求。

也就是牺牲数据的一致性来满足系统的高可用性，系统中一部分数据不可用或者不一致时，仍需要保持系统整体“主要可用”。

- `基本可用`：基本可用是指分布式系统在出现不可预知故障的时候，允许损失部分可用性。但是，这绝不等价于系统不可用。
- `软状态`：软状态指允许系统中的数据存在中间状态（**CAP 理论中的数据不一致**），并认为该中间状态的存在不会影响系统的整体可用性，即允许系统在不同节点的数据副本之间进行数据同步的过程存在延时。
- `最终一致性`：最终一致性强调的是系统中所有的数据副本，在经过一段时间的同步后，最终能够达到一个一致的状态。因此，最终一致性的本质是需要系统保证最终数据能够达到一致，而不需要实时保证系统数据的强一致性。

### Basically Available(基本可用)

-   `对响应上时间的妥协`：正常情况下，一个在线搜索引擎需要在0.5秒之内返回给用户相应的查询结果，但由于出现故障（比如系统部分机房发生断电或断网故障），查询结果的响应时间增加到了1~2秒。
-   `对功能损失的妥协`：正常情况下，在一个电子商务网站（比如淘宝）上购物，消费者几乎能够顺利地完成每一笔订单。但在一些节日大促购物高峰的时候（比如双十一、双十二），由于消费者的购物行为激增，为了保护系统的稳定性（或者保证一致性），部分消费者可能会被引导到一个降级页面

### Soft state（软状态）

- 原子性（硬状态） -> 要求多个节点的数据副本都是一致的,这是一种"硬状态"
- 软状态（弱状态） -> 允许系统中的数据存在中间状态,并认为该状态不影响系统的整体可用性,即允许系统在多个不同节点的数据副本存在数据延迟。

### Eventually consistent（最终一致性）

系统能够保证在没有其他新的更新操作的情况下，数据最终一定能够达到一致的状态，因此所有客户端对系统的数据访问最终都能够获取到最新的值。
上面说软状态，然后不可能一直是软状态，必须有个时间期限。在期限过后，应当保证所有副本保持数据一致性。从而达到数据的最终一致性。这个时间期限取决于网络延时，系统负载，数据复制方案设计等等因素。

### 总结

总的来说，BASE 理论面向的是大型高可用可扩展的分布式系统，和传统事务的 ACID 是**相反的**，它完全不同于 ACID 的强一致性模型，而是**通过牺牲强一致性**来获得可用性，并允许数据在一段时间是不一致的。

## 分布式id

### uuid

为了保证UUID的唯一性，规范定义了包括网卡MAC地址、时间戳、名字空间（Namespace）、随机或伪随机数、时序等元素，以及从这些元素生成UUID的算法。UUID的复杂特性在保证了其唯一性的同时，意味着只能由计算机生成。

`UUID`的生成简单到只有一行代码，输出结果 `c2b8c2b9e46c47e3b30dca3b0d447718`，但UUID却并不适用于实际的业务需求。像用作订单号`UUID`这样的字符串没有丝毫的意义，看不出和订单相关的有用信息；而对于数据库来说用作业务`主键ID`，它不仅是太长还是字符串，存储性能差查询也很耗时，所以不推荐用作`分布式ID`。

### 雪花算法

雪花算法的原理就是生成一个的 64 位比特位的 long 类型的唯一 id。

-   最高 1 位固定值 0，因为生成的 id 是正整数，如果是 1 就是负数了。
-   接下来 41 位存储毫秒级时间戳，2^41/(1000\*60\*60\*24\*365)=69，大概可以使用 69 年。
-   再接下 10 位存储机器码，包括 5 位 datacenterId 和 5 位 workerId。最多可以部署 2^10=1024 台机器。
-   最后 12 位存储序列号。同一毫秒时间戳时，通过这个递增的序列号来区分。即对于同一台机器而言，同一毫秒时间戳下，可以生成 2^12=4096 个不重复 id。

雪花算法有如下缺点：

-   依赖服务器时间，服务器时钟回拨时可能会生成重复 id。算法中可通过记录最后一个生成 id 时的时间戳来解决，每次生成 id 之前比较当前服务器时钟是否被回拨，避免生成重复 id。

### mysql自增id

当我们需要一个ID的时候，向表中插入一条记录返回`主键ID`，但这种方式有一个比较致命的缺点，访问量激增时MySQL本身就是系统的瓶颈，用它来实现分布式服务风险比较大，不推荐！
集群就设置不同的起始值和步长，奇数偶数

### redis，incr自增id

`Redis`也同样可以实现，原理就是利用`redis`的 `incr`命令实现ID的原子性自增。

用`redis`实现需要注意一点，要考虑到redis持久化的问题。`redis`有两种持久化方式`RDB`和`AOF`

-   `RDB`会定时打一个快照进行持久化，假如连续自增但`redis`没及时持久化，而这会Redis挂掉了，重启Redis后会出现ID重复的情况。
-   `AOF`会对每条写命令进行持久化，即使`Redis`挂掉了也不会出现ID重复的情况，但由于incr命令的特殊性，会导致`Redis`重启恢复的数据时间过长。

## Raft算法

- **Leader**：接受客户端请求，并向Follower同步请求日志，当日志同步到大多数节点上后告诉Follower提交日志。
- **Follower**：接受并持久化Leader同步的日志，在Leader告之日志可以提交之后，提交日志。
- **Candidate**：Leader选举过程中的临时角色。

每个结点都会有一个定时器，每次收到来自 Leader 的信息就会更新该定时器。

如果定时器超时，说明一段时间内没有收到 Leader 的消息，那么就可以认为 Leader 已死或者不存在，那么该结点就会转变成 Candidate，意思为准备竞争成为 Leader。

成为 Candidate 后结点会向所有其他结点发送请求投票的请求（RequestVote），其他结点在收到请求后会判断是否可以投给他并返回结果。Candidate 如果收到了半数以上的投票就可以成为 Leader，成为之后会立即并在任期内定期发送一个心跳信息通知其他所有结点新的 Leader 信息，并用来重置定时器，避免其他结点再次成为 Candidate。

Raft算法是一种用于分布式系统中实现一致性的算法。它是一种领导者选举算法，可以确保在任何时刻只有一个领导者，并且领导者可以在不丢失任何提交的日志条目的情况下崩溃或重启。

Raft算法中的节点分为三种角色：领导者、跟随者和候选人。当集群初始化或领导者崩溃时，节点将从跟随者转换为候选人。候选人首先请求投票，并在投票超过半数的情况下成为新的领导者。在新的领导者选举完成之后，它将负责接受客户端的请求并向所有跟随者复制日志条目。

## 分布式事务 

分布式系统会把一个应用系统拆分为可独立部署的多个服务，因此需要服务与服务之间远程协作才能完成事务操作，这种分布式系统环境下由不同的服务之间通过网络远程协作完成事务称之为**分布式事务**，例如用户注册送积分事务、创建订单减库存事务，银行转账事务等都是分布式事务。

分布式事务是指存在多个跨库事务的事务一致性问题，或者在分布式架构下由多个应用节点组成的多个事务之间的事务一致性问题。需要保证在多个参与者之间的数据一致性与原子性

### XA模型 X/Open DTP模型

X/Open **DTP(Distributed Transaction Process)** 是一个分布式事务模型。这个模型主要使用了两段提交(2PC - Two-Phase-Commit)来保证分布式事务的完整性。

在X/Open **DTP(Distributed Transaction Process)**模型里面，有三个角色：

- AP: Application，**应用程序**。也就是业务层。哪些操作属于一个事务，就是AP定义的。
- TM: Transaction Manager，**事务管理器**。接收AP的事务请求，对全局事务进行管理，管理事务分支状态，协调RM的处理，通知RM哪些操作属于哪些全局事务以及事务分支等等。这个也是整个事务调度模型的核心部分。
- RM：Resource Manager，**资源管理器**。一般是数据库，也可以是其他的资源管理器，如消息队列(如JMS数据源)，文件系统等。

### 2PC两阶段提交

1.  准备阶段（Prepare phase）：事务管理器给每个参与者发送 Prepare 消息，每个数据库参与者在本地执行事务，并写本地的 Undo/Redo 日志，此时事务没有提交。（Undo 日志是记录修改前的数据，用于数据库回滚，Redo 日志是记录修改后的数据，用于提交事务后写入数据文件）==锁定资源但不提交==
2.  提交阶段（commit phase）：如果事务管理器收到了参与者的执行失败或者超时消息时，直接给每个参与者发送回滚（Rollback）消息；否则，发送提交（Commit）消息；参与者根据事务管理器的指令执行提交或者回滚操作，并释放事务处理过程中使用的锁资源。注意：**必须在最后阶段释放锁资源**。

#### 缺点

1. 所有参与者需要等待阻塞事务管理器的提交命令
2. 一旦事务协调者宕机或者发生网络抖动，会让参与者一直处于锁定资源的状态或者只有一部分参与者提交成功，导致数据的不一致
3. 最后阶段才释放锁资源，导致系统吞吐量降低

#### 如何处理通信故障

1. 超时机制 ：在2PC协议中，每个阶段都有一个预定的超时时间。如果在超时时间内没有收到响应，协调者将7会进行相应的处理。例如，如果在第一阶段中协调者无法收到参与者的响应，它可以将参与者视为失败，并通知所有其他参与者回滚事务。
2. 心跳机制：协调者可以定期向参与者发送心跳消息，以检测参与者的状态。如果协调者在一段时间内没有收2到参与者的响应，它可以将参与者视为失败并进行相应的处理
3. 预备性提交：在第一阶段中，协调者可以请求参与者进行预备性提交，并在得到所有参与者的预备性提交确认后，将事务提交请求发给所有参与者。如果在第二阶段中，协调者无法收到某个参与者的确认消息，则可以向该参与者发送回滚请求。
4. 备份协调者:在2PC中，可以使用备份协调者来提高系统的可靠性。备份协调者可以监控协调者的状态，并在协调者失效时接替其工作
5. 消息队列:参与者可以将事务日志写入消息队列，协调者可以从消息队列中获取事务日志，并进行相应的处理。如果协调者在处理事务时失效，备份协调者可以从消息队列中获取未处理的事务日志，并继续进行处理。

### 3PC

三阶段提交协议是在2PC的基础上进行了优化，通过增加一个额外的阶段来==减少同步阻塞和单点故障的风险==。

上边提到两段提交，当协调者崩溃时，参与者不能做出最后的选择，就会一直保持阻塞锁定资源。

2PC中只有协调者有超时机制，3PC在协调者和参与者中都引入了超时机制，协调者出现故障后，参与者就不会一直阻塞。而且在第一阶段和第二阶段中又插入了一个准备阶段，保证了在最后提交阶段之前各参与节点的状态是一致的。

1. CanCommit阶段: 协调者向所有参与者发送CanCommit请求，询问它们是否可以提交事务。参与者收到请求后，根据自身状态回复协调者，如果可以提交事务，则返回"Yes";如果不能提交事务，则返回"No"
2. PreCommit阶段: 协调者根据参与者的回复来决定整个事务是继续还是放弃。如果所有参与者都返回"Yes"，协调者会向它们发送PreCommit请求;如果有参与者返回“No"，协调者会向所有参与者发送Abort请求。参与者收到PreCommit请求后，会执行事务操作，但不会立即提交，而是将其置于“就绪”状态，等待协调者的下一步指不。
3. DoCommit阶段: 协调者会根据参与者的状态决定事务的最终结果。如果所有参与者都处于就绪状态，协调者会向它们发送DoCommit请求，要求它们提交事务;如果有参与者未处于就绪状态，协调者会向所有参与者发送Abort请求，要求它们放弃事

最后一个阶段如果因为超时没有收到协调者的最终提交命令，仍会提交因为在上一个阶段已经确认了所有节点状态一致

#### 缺点：

虽然3PC用超时机制，解决了协调者故障后参与者的阻塞问题，但与此同时却多了一次网络通信，性能上反而变得更差，也不太推荐。

### TCC

TCC协议是一种基于补偿机制的分布式事务协议，他通过三个阶段实现事务的原子性和一致性。

所谓的 TCC编程模式，也是两阶段提交的一个变种，不同的是TCC为在业务层编写代码实现的两阶段提交。TCC分别指 `Try`、`Confirm`、`Cancel` ，一个业务操作要对应的写这三个方法。

以下单扣库存为例，`Try` 阶段去占库存，`Confirm` 阶段则实际扣库存，如果库存扣减失败 `Cancel` 阶段进行回滚，释放库存。

TCC 不存在资源阻塞的问题，因为每个方法都直接进行事务的提交，一旦出现异常通过则 `Cancel` 来进行回滚补偿，这也就是常说的补偿性事务。

原本一个方法，现在却需要三个方法来支持，可以看到 TCC 对业务的侵入性很强，而且这种模式并不能很好地被复用，会导致开发量激增。还要考虑到网络波动等原因，为保证请求一定送达都会有重试机制，所以考虑到接口的幂等性。

1. Try（尝试）：在TCC模式中，事务的第一步是尝试（Try）阶段。在这个阶段，系统会预留和检查所有参与者所需的资源，并执行一些预操作。这个阶段主要用于检查业务规则、资源可用性和执行一些必要的准备工作。
2. Confirm（确认）：如果所有的参与者在尝试阶段都成功执行，并且没有发生任何错误，那么事务会进入确认（Confirm）阶段。在这个阶段，系统会执行真正的业务逻辑，将之前预留的资源进行实际的提交。确认阶段的操作应该是幂等的，因为在某些情况下可能需要多次执行确认操作。
3. Cancel（取消）：如果任何一个参与者在尝试阶段或确认阶段发生错误，事务会进入取消（Cancel）阶段。在这个阶段，系统会回滚和释放之前预留的资源，以确保事务的回滚操作。取消阶段的操作也应该是幂等的，以防止多次取消操作引起的问题。

#### 缺点

1. 实现复杂，每个事务都需要进行预处理
2. 性能损失：每个参与方都需要执行三个阶段的操作
3. 并发度低：每个阶段都会进行锁定操作
